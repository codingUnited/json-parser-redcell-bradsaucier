# JSON Parser - Red Cell Sweep
[Annex: Parser Design](./PARSER_DESIGN_ANNEX.md)

[![json-parser-redcell-sweep](https://github.com/codingUnited/Red-Cell-Sweep/actions/workflows/ci.yml/badge.svg)](https://github.com/codingUnited/Red-Cell-Sweep/actions/workflows/ci.yml)
[![Python 3.10+](https://img.shields.io/badge/python-3.10%2B-blue)](https://www.python.org)
[![License MIT](https://img.shields.io/badge/license-MIT-green)](LICENSE)

> **Mission Profile**  
> Zero-dependency JSON validator. Sub-millisecond on small payloads. Depth-capped for hostile input. Built and documented like a nine-line medevac: short, complete, and deadly accurate.

**Author**  
Bradley Saucier  
B.S. Candidate, Computer Science (STEM Project Management), Southern New Hampshire University  
B.A. Economics, Columbia University  
A.A.S., Community College of the Air Force

---

## Table of Contents
1. [Why This Parser](#why-this-parser)
2. [CLI Flags](#cli-flags)
3. [Streaming Threshold Logic](#streaming-threshold-logic)
4. [Architecture](#architecture)
5. [Performance Notes](#performance-notes)
6. [AST Considerations](#ast-considerations)
7. [How To Run](#how-to-run)
8. [References](#references)
9. [License](#license)

---

## Why This Parser

This design is purpose-built for competitive parsing environments where clarity, control, and speed matter.

* **Pruned Lexer** - one compiled regex with named groups walks memory in predictable strides, reducing branch mispredictions and keeping cache lines hot [craftinginterpreters.com, Scanning].
* **Recursive-descent Core** - ideal for JSONâ€™s expression-free, LL(1)-friendly grammar [geeksforgeeks.org, Recursive Descent Parser]. No table generators or opaque parser frameworks.
* **Depth Guard** - 19-level limit mirrors `JSON_checker` as a proven countermeasure against resource-exhaustion payloads [RFC 8259; hypertextbookshop.com, Parser Error Handling].
* **Optional Duplicate Keys** - operator decides whether speed or policy purity takes priority.
* **Tight LookAhead** - single-slot pushback supports LL(1) control flow without the overhead of full buffering [geeksforgeeks.org, Top Down Parsing].
* **CI Pipeline** - badge above confirms every commit clears five escalating validation stages.

---

## CLI Flags

| Flag | Default | Purpose |
|------|---------|---------|
| `--max-depth N` | 19 | Abort if nesting exceeds *N*. Enforces hard ceiling against deep-structure attacks. |
| `--allow-dup-keys` | off | Skip duplicate-key detection for faster bulk ingest. |
| `--debug` | off | Prints lexer tokens then exits. Rapid isolation of malformed input. |
| `--streaming-threshold BYTES` | 262144 | Payloads above this size will trigger stream mode in a future update. |

---

## Streaming Threshold Logic

Small payloads run entirely in memory for maximum speed. When input size exceeds `--streaming-threshold`, the parser will (in a future release) shift to chunked reads with equivalent validation guarantees. This approach caps memory footprint without ceding control to slower external libraries.

---

## Architecture

```
          +------------------+
 raw text |  Regex Scanner   |  Tokens
          +--------+---------+
                   |
                   v
          +------------------+     One-slot lookahead
          |  LookAhead Ring  |----> Depth guard, duplicate key policy
          +------------------+
                   |
                   v
          +------------------+
          | Recursive-Descent|  ->  Native Python dict / list / value
          |    Core          |
          +------------------+
```

**Design Doctrine**: Tokenization and structural parsing are cleanly separated [tutorialspoint.com, Compiler Design Tutorial]. The lexer handles all type conversion up front; the parser deals only in native Python types. This keeps the control graph lean and predictable.

---

## Performance Notes

* `twitter.json` (631 KB) validates in **5.4 ms median** on an M1 Pro.
* Lexer accounts for ~55 percent of cycles - exactly where a tight regex pays off.
* Disabling duplicate-key detection improves throughput by ~7 percent. At that point, ingest speed is bounded by disk I/O.

---

## Testing

Every push triggers the full CI pipeline shown in the badge at the top of this README. The suite runs under `pytest` with coverage reporting enabled. It includes:

- **Harness tests** for each of the five challenge steps, validating both valid and invalid JSON.
- **Unit tests** targeting:
  - String validation, including all escape sequences, surrogate pairs, and error reporting for malformed sequences.
  - Nesting depth enforcement and duplicate key policy behavior.
  - Structural correctness of arrays and objects, including trailing data detection.
  - CLI operation, flag parsing, and placeholder streaming mode behavior.
- **Performance sanity checks** on representative payloads.

**Current coverage**: ~94 percent overall. The remaining uncovered lines are primarily in streaming placeholders and defensive branches.

To run the full suite locally:

```bash
pytest -q --disable-warnings --cov=. --cov-report=term-missing
```

To run a specific test file:

```bash
pytest tests/unit/test_strings.py
```

For quick manual validation:

```bash
echo '{"a":1}' | python json_parser.py /dev/stdin
```

Exit code **0** means pass. Any non-zero means the parser flagged a syntax failure.
## Streaming placeholder status
The implementation makes the current status explicit:
- The `--streaming-threshold` flag exists, but chunked lexing is not implemented yet.
- `_cli` includes a comment describing the streaming roadmap.
- When the input exceeds the threshold, the tool prints `OK (stream mode placeholder)` to signal that streaming mode is not active yet.

---

## AST Considerations

This parser terminates at Python-native structures (dict, list, value) rather than building an explicit Abstract Syntax Tree (AST). In a language interpreter, an AST is often the canonical intermediate representation, omitting syntactic sugar while preserving semantic structure [meegle.com, Abstract Syntax Tree Creation; blog.trailofbits.com, The Life and Times of an AST]. For JSON, native structures are the semantic form, making a separate AST layer unnecessary for validation-only use cases.

---

## How To Run

```bash
# Single file validation
python json_parser.py payload.json

# Increase depth sensitivity
python json_parser.py payload.json --max-depth 8

# Bulk ingest, skip duplicate key checks
python json_parser.py payload.json --allow-dup-keys
```

Exit code **0** means pass. Any non-zero indicates syntax failure.

---

## References

1. geeksforgeeks.org - Recursive Descent Parser  
2. cs.rochester.edu - Recursive-Descent Parsing  
3. craftinginterpreters.com - Scanning  
4. RFC 8259 - The JavaScript Object Notation (JSON) standard  
5. hypertextbookshop.com - Parser Error Handling and Recovery  
6. tutorialspoint.com - Compiler Design Tutorial  
7. meegle.com - Abstract Syntax Tree Creation  
8. blog.trailofbits.com - The Life and Times of an AST  

---

## License

MIT. Use it, fork it, but keep the badge flying.

//viper1//
